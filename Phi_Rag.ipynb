{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4a67ab-41c5-4920-a510-fe7fdcba1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the main libraries for setting up code to interact with LLM\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_huggingface import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5fb33c-81d9-46ef-a9c8-0e37ddc64303",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Question: {question}\\\n",
    "Answer: Letâ€™s work this out in a step by step way to be sure we have the right answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2826c24e-8e85-4b77-bc5e-bb43bc5484f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "n_gpu_layers = 1 # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 4 # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e103f0-d6f2-4c66-95c5-84e4c8a1d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19bfa8f7-2141-4c49-8af6-099d976a8381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8aad70f76d447cb2bcc3d5e1611f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "model_name = 'microsoft/phi-2'\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model_name,\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    temperature=0.6,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b46be-93ca-4415-b5d2-99e8a1a1b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure the model path is correct for your system!\n",
    "# llm = LlamaCpp(\n",
    "#     model_path=r\"C:\\Users\\ritap\\.cache\\lm-studio\\models\\lmstudio-community\\Meta-Llama-3-8B-Instruct-GGUF\\Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n",
    "#     n_gpu_layers=n_gpu_layers, n_batch=n_batch,\n",
    "#     n_ctx = 3000,\n",
    "#     temperature=0.0,\n",
    "#     max_tokens=2000,\n",
    "#     top_p=1,\n",
    "#     callback_manager=callback_manager,\n",
    "#     verbose=True, # Verbose is required to pass to the callback manager\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd9def-71ba-43d7-8271-d2cc7044b849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritap\\anaconda3\\envs\\Langchain\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ritap\\anaconda3\\envs\\Langchain\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================================== Outcome from model =======================================>\n"
     ]
    }
   ],
   "source": [
    "#Question for LLM\n",
    "question = \"Which are the top 5 companies in world with their revenue in table format?\"\n",
    "\n",
    "#providing the results\n",
    "print(\"<====================================== Outcome from model =======================================>\")\n",
    "llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf94678-2b15-49f6-a6ba-744f29a78e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
