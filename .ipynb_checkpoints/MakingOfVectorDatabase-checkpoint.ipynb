{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1ba463-7b3f-422d-9243-8cb11231dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USER_AGENT'] = 'myagent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c88d04-9741-4286-bdb6-f697bb0507bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7173ae84-2ba5-4673-8e2a-e0ae0fcc1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings # other embeddings available \n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ead250-f9c6-4cb3-9d79-99641ab71a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fbebe-aedc-4511-aec7-62f7889e79da",
   "metadata": {},
   "source": [
    "## Vector database from a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4240c5-1bc0-4eaa-b365-eed6e6565a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"I am Rito\", \"I am a male\", \"I work as a data scientist\", \"My favourite game is cricket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9b8ab3-c624-4864-996c-3d9501897158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422be843-6067-4944-b96c-a03feaef2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the data in Vector Store\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "# create embeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# database\n",
    "vector_database = Chroma.from_texts(texts=data, embedding=embedding) # takes a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01132b46-ecdc-4e4a-99d5-babb1ddf5028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1c46b5b8eb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5acae-4599-4174-89f1-82d63d45c649",
   "metadata": {},
   "source": [
    "## Vector database from a large pice of text( string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e30d5cbd-5f81-49ec-933f-717b43921da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r\"I am Rito. I am a male. I work as a data scientist. My favourite game is cricket.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f450158-a25f-4b74-9cfb-087dd68129a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 12, which is longer than the specified 10\n",
      "Created a chunk of size 27, which is longer than the specified 10\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(separator='.', chunk_size=10, chunk_overlap=0)\n",
    "data = text_splitter.split_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "537c01fa-4f49-4889-9141-9bba25fd46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(CharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60411e67-fefa-4da8-8b0c-1bdd60fc6292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Rito',\n",
       " 'I am a male',\n",
       " 'I work as a data scientist',\n",
       " 'My favourite game is cricket']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d73d1d3-b11c-4b75-b186-b7711d97ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the data in Vector Store\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "# create embeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# database\n",
    "vector_database = Chroma.from_texts(texts=data, embedding=embedding) # takes a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ffc1fa0-967e-49ac-a4b3-e31ef2b649ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1c4697bb400>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6deb67-2d71-4530-9ba8-32c69970af8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d8356-1cd4-4d43-92e0-4b9cf13e3ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4873b857-f72b-4882-b6ea-2a1ae6bac075",
   "metadata": {},
   "source": [
    "## From a text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b024faf0-bbb0-4988-a833-011a5d1d166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1153, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "raw_documents = TextLoader('wikipedia.txt').load()\n",
    "text_splitter = CharacterTextSplitter(separator = '.', chunk_size=10, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "923ddf24-d7be-4f91-a836-61d3edf744ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'wikipedia.txt'}, page_content='Deep learning is the subset of machine learning methods based on neural networks with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\\n\\nDeep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, in particular the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low quality models for that purpose.[6]\\n\\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[7]'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Importantly, a deep learning process can learn which features to optimally place in which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate upon. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[8][2]'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[9] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[10] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Deep learning architectures can be constructed with a greedy layer-by-layer method.[11] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[8]\\n\\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[8][12]\\n\\nThe term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[14][15] Although the history of its appearance is apparently more complicated.[16]\\n\\nInterpretations\\nDeep neural networks are generally interpreted in terms of the universal approximation theorem[17][18][19][20][21] or probabilistic inference.[22][23][8][9][24]'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content=\"The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[17][18][19][20] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[17] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[18] Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.[25][26]\\n\\nThe universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[21] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\"),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='The probabilistic interpretation[24] derives from the field of machine learning. It features inference,[23][7][8][9][12][24] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[24] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[27]'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content=\"Frank Rosenblatt (1958)[28] proposed the perceptron, a multilayer perceptron (MLP) with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book also introduced variants and computer experiments, including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).[29]:â€Šsection 16â€Š Some consider that the 1962 book developed and explored all of the basic ingredients of the deep learning systems of today.[30]\\n\\nGroup method of data handling, a method to train arbitrarily deep neural networks was published by Alexey Ivakhnenko and Lapa in 1967, which they regarded as a form of polynomial regression,[31] or a generalization of Rosenblatt's perceptron.[32] A 1971 paper described a deep network with eight layers trained by this method.[33]\"),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content=\"The first deep learning multilayer perceptron trained by stochastic gradient descent[34] was published in 1967 by Shun'ichi Amari.[35] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.[36] Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\"),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[37] to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt,[29] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.[38] The modern form of backpropagation was developed multiple times in early 1970s. The earliest published instance was Seppo Linnainmaa\\'s master thesis (1970).[39][40][36] Paul Werbos developed it independently in 1971,[41] but had difficulty publishing it until 1982.[42] In 1986, David E. Rumelhart et al. popularized backpropagation.[43][44]'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='1980s-2000s\\nDeep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980, though not trained by backpropagation.[45] In 1969, he also introduced the ReLU (rectified linear unit) activation function.[25][36] The rectifier has become the most popular activation function for CNNs and deep learning in general.[46]'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Backpropagation-trained CNN saw early successes in 1980s, such as for alphabet recognition[47] and on an optical computing hardware.[48] The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used both weight sharing and backpropagation.[49] In 1989, Yann LeCun et al. created the LeNet, which applied backpropagation to a CNN for recognizing handwritten ZIP codes on mail. Training required 3 days.[50] Other examples included medical image object segmentation[51] and breast cancer detection in mammograms.[52] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images.[53]'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Recurrent neural networks (RNN) were also developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986)[54] and the Elman network (1990),[55] which applied RNN to study problems in cognitive psychology.'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Sepp Hochreiter\\'s diploma thesis (1991)[56] proposed the neural history compressor, and identified and analyzed the vanishing gradient problem.[56][57] In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.[58][59] Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995.[60] LSTM can learn \"very deep learning\" tasks[9] with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999,[61] which became the standard RNN architecture.'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='During 1985â€“1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,[62] restricted Boltzmann machine,[63] Helmholtz machine,[64] and the wake-sleep algorithm.[65] These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 [66]). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.[67]'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years.[68][69][70] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[71] Key difficulties have been analyzed, including gradient diminishing[56] and weak temporal correlation structure in neural predictive models.[72][73] Additional difficulties were the lack of training data and limited computing power.'),\n",
       " Document(metadata={'source': 'wikipedia.txt'}, page_content='Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\\'s NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark.[74][75] It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[76]\\n\\nThe principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s,[75] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[77]')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099246e-cd26-4027-b88a-dad5cb3c1938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
